{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360239f-6e7d-451e-9f82-88a3c3674a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data collection\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Define a new folder for A-Z data\n",
    "DATA_DIR = 'D:/ML_go/Data_AZ'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Collect data for A-Z (26 letters)\n",
    "classes = [chr(i) for i in range(ord('A'), ord('Z')+1)]\n",
    "dataset_size = 100  # Number of images per letter\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "for letter in classes:\n",
    "    letter_dir = os.path.join(DATA_DIR, letter)\n",
    "    os.makedirs(letter_dir, exist_ok=True)  # Create folder for each letter\n",
    "\n",
    "    print(f'\\nðŸ“· Collecting data for letter: {letter}')\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Show the current letter in the center of the frame\n",
    "        cv2.putText(frame, f'Letter: {letter}', (100, 100), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        cv2.putText(frame, 'Press \"Q\" to start!', (100, 200), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Sign Language Data Collection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break  # Start capturing images when 'Q' is pressed\n",
    "\n",
    "    counter = 0\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Show progress on screen\n",
    "        cv2.putText(frame, f'Capturing {letter}: {counter+1}/{dataset_size}', (50, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Sign Language Data Collection', frame)\n",
    "        cv2.imwrite(os.path.join(letter_dir, f'{counter}.jpg'), frame)\n",
    "\n",
    "        counter += 1\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a53e11-45fd-407d-891f-a794942574b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hand landmarks extracted and saved as letters1.pkl\n"
     ]
    }
   ],
   "source": [
    "#alphabet preprocessing\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# Path to dataset\n",
    "DATA_DIR = 'D:/ML_go/Data_AZ'\n",
    "\n",
    "# Labels (A-Z)\n",
    "classes = [chr(i) for i in range(ord('A'), ord('Z')+1)]\n",
    "\n",
    "# Lists to store data\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for letter in classes:\n",
    "    letter_dir = os.path.join(DATA_DIR, letter)\n",
    "\n",
    "    for image_file in os.listdir(letter_dir):\n",
    "        image_path = os.path.join(letter_dir, image_file)\n",
    "\n",
    "        # Read and convert image\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process with MediaPipe\n",
    "        results = hands.process(img_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                x_ = [lm.x for lm in hand_landmarks.landmark]\n",
    "                y_ = [lm.y for lm in hand_landmarks.landmark]\n",
    "\n",
    "                # Normalize landmarks\n",
    "                min_x, min_y = min(x_), min(y_)\n",
    "                landmarks = [(x - min_x, y - min_y) for x, y in zip(x_, y_)]\n",
    "                \n",
    "                # Flatten into a feature vector\n",
    "                feature_vector = np.array(landmarks).flatten()\n",
    "                \n",
    "                # Store data\n",
    "                data.append(feature_vector)\n",
    "                labels.append(letter)\n",
    "\n",
    "# Save processed data\n",
    "with open(\"letters1.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"data\": data, \"labels\": labels}, f)\n",
    "\n",
    "print(\"âœ… Hand landmarks extracted and saved as letters1.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d3347bb-5639-4e49-b357-d5b8b3d4522f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained with accuracy: 1.00\n",
      "ðŸŽ¯ Model saved as letter_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#alphabet trainig\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load preprocessed data\n",
    "with open(\"letters1.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "data = np.array(dataset[\"data\"])\n",
    "labels = np.array(dataset[\"labels\"])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… Model trained with accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"letter_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"ðŸŽ¯ Model saved as letter_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1005b1-5f5d-47ab-a23a-ed467f0c17f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#alphabet prediction\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load the trained model\n",
    "with open(\"letter_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to RGB and process with MediaPipe\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    # Prepare data for prediction\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            x_ = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y_ = [lm.y for lm in hand_landmarks.landmark]\n",
    "\n",
    "            # Normalize landmarks\n",
    "            min_x, min_y = min(x_), min(y_)\n",
    "            landmarks = [(x - min_x, y - min_y) for x, y in zip(x_, y_)]\n",
    "            \n",
    "            # Convert to a feature vector\n",
    "            feature_vector = np.array(landmarks).flatten().reshape(1, -1)\n",
    "\n",
    "            # Predict the letter\n",
    "            prediction = model.predict(feature_vector)[0]\n",
    "\n",
    "            # Display prediction\n",
    "            cv2.putText(frame, f'Prediction: {prediction}', (50, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Sign Language Prediction\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9c419-cc0c-4ba2-be4f-0eae38c33347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89b91f-133d-4a58-978f-405adb135cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#word data collection\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define dataset storage path\n",
    "DATASET_PATH = \"sign_data\"  # Folder where all images will be stored\n",
    "\n",
    "# List of 10 words\n",
    "WORDS = [\"Hello\", \"Yes\", \"No\", \"ThankYou\", \"Sorry\", \"Please\", \"Help\", \"Good\", \"Bad\", \"Stop\"]\n",
    "\n",
    "# Initialize camera\n",
    "cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
    "print(\"Press 'S' to start collecting images.\")\n",
    "print(\"Press 'Q' to quit at any time.\")\n",
    "\n",
    "for sign_name in WORDS:\n",
    "    sign_folder = os.path.join(DATASET_PATH, sign_name)\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(sign_folder):\n",
    "        os.makedirs(sign_folder)\n",
    "\n",
    "    print(f\"\\nGet ready for '{sign_name}'. Press 'S' to start collecting images.\")\n",
    "    \n",
    "    # Wait for 'S' to start collecting for the current word\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.putText(frame, f\"Ready for {sign_name} - Press 'S' to start\", (10, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Data Collection\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('s'):\n",
    "            break\n",
    "        elif key == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            exit()\n",
    "\n",
    "    count = 0\n",
    "    print(f\"Collecting images for '{sign_name}'... Press 'Q' to skip.\")\n",
    "\n",
    "    while count < 100:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Display progress\n",
    "        cv2.putText(frame, f\"{sign_name} ({count}/100)\", (10, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Data Collection\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Capture images automatically\n",
    "        img_path = os.path.join(sign_folder, f\"{count}.jpg\")\n",
    "        cv2.imwrite(img_path, frame)\n",
    "        count += 1\n",
    "\n",
    "        # Skip to next word if 'Q' is pressed\n",
    "        if key == ord('q'):\n",
    "            print(f\"Skipping '{sign_name}'...\")\n",
    "            break\n",
    "\n",
    "    print(f\"Completed collection for '{sign_name}'.\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Data collection finished for all words!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28d61d0c-7492-430a-9839-fc847fed9fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data saved as words_data.pkl\n"
     ]
    }
   ],
   "source": [
    "#word preprocessing\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(DATA_DIR, labels_list, output_file):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in labels_list:\n",
    "        folder_path = os.path.join(DATA_DIR, label)\n",
    "        \n",
    "        for image_file in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            img = cv2.imread(image_path)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            results = hands.process(img_rgb)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    x_ = [lm.x for lm in hand_landmarks.landmark]\n",
    "                    y_ = [lm.y for lm in hand_landmarks.landmark]\n",
    "                    \n",
    "                    # Normalize landmarks\n",
    "                    min_x, min_y = min(x_), min(y_)\n",
    "                    landmarks = [(x - min_x, y - min_y) for x, y in zip(x_, y_)]\n",
    "                    \n",
    "                    # Flatten into a feature vector\n",
    "                    feature_vector = np.array(landmarks).flatten()\n",
    "                    \n",
    "                    data.append(feature_vector)\n",
    "                    labels.append(label)\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump({\"data\": data, \"labels\": labels}, f)\n",
    "    print(f\"âœ… Data saved as {output_file}\")\n",
    "\n",
    "# Paths\n",
    "WORD_DATA_DIR = \"sign_data\"  # Path where word images are stored\n",
    "words = [\"Hello\", \"Yes\", \"No\", \"ThankYou\", \"Sorry\", \"Please\", \"Help\", \"Good\", \"Bad\", \"Stop\"]\n",
    "\n",
    "# Preprocess and save\n",
    "preprocess_data(WORD_DATA_DIR, words, \"words_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79723471-368a-46a7-a226-d13bdb11b7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained with accuracy: 1.00\n",
      "âœ… Model saved as sign_language_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#word training\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load preprocessed word data\n",
    "with open(\"words_data.pkl\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "X = np.array(data_dict[\"data\"])\n",
    "y = np.array(data_dict[\"labels\"])\n",
    "\n",
    "# Split data into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… Model trained with accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"sign_language_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"âœ… Model saved as sign_language_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc5fcdb-cf9a-4387-9adc-cf642aebe8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#real time word prediction\n",
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Load trained model\n",
    "with open(\"sign_language_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip frame horizontally for a natural view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process hand landmarks\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            x_ = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y_ = [lm.y for lm in hand_landmarks.landmark]\n",
    "\n",
    "            # Normalize coordinates\n",
    "            min_x, min_y = min(x_), min(y_)\n",
    "            landmarks = [(x - min_x, y - min_y) for x, y in zip(x_, y_)]\n",
    "\n",
    "            # Convert to numpy array\n",
    "            feature_vector = np.array(landmarks).flatten().reshape(1, -1)\n",
    "\n",
    "            # Predict using trained model\n",
    "            prediction = model.predict(feature_vector)[0]\n",
    "\n",
    "            # Display prediction\n",
    "            cv2.putText(frame, f'Prediction: {prediction}', (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show webcam feed\n",
    "    cv2.imshow(\"Sign Language Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0bbb53-f98c-48dc-81b6-86bebb049eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
